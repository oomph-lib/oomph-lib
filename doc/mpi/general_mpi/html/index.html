<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
<title>oomph-lib: Parallel processing</title>
<link rel="apple-touch-icon" sizes="57x57" href="../../../figures/apple-touch-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="../../../figures/apple-touch-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="../../../figures/apple-touch-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="../../../figures/apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="../../../figures/apple-touch-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="../../../figures/apple-touch-icon-120x120.png">
<link rel="icon" type="image/png" href="../../../figures/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="../../../figures/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="../../../figures/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="../../../figures/manifest.json">
<link rel="mask-icon" href="../../../figures/safari-pinned-tab.svg" color="#008000">
<link rel="shortcut icon" href="../../../figures/favicon.ico">
<meta name="msapplication-TileColor" content="#00a300">
<meta name="msapplication-config" content="../../../figures/browserconfig.xml">
<meta name="theme-color" content="#008000">
<link href="http://fonts.googleapis.com/css?family=Open+Sans:400,300,600" rel="stylesheet" type="text/css">
<!-- Doxygen css-->
<!-- <link rel="stylesheet" type="text/css" href="doxygen.css"> -->
<!-- Bootstrap -->
<link href="../../../css/bootstrap.css" rel="stylesheet">
<!-- oomph-lib specific overrides -->
<link rel="stylesheet" type="text/css" href="../../../css/oomph_header.css">
</head>
<body>
<nav class="navbar navbar-default">
<div class="container">
<div class="container-fluid">
  <!-- Brand and toggle get grouped for better mobile display -->
  <div class="navbar-header">
    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="../../../html/index.html"><img alt="oomph-lib" src="../../../figures/oomph_logo.png"></a>
  </div>
  <!-- Collect the nav links, forms, and other content for toggling -->
  <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
    <ul class="nav navbar-nav">          
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Documentation <span class="caret"></span></a>
        <ul class="dropdown-menu">
          <li class="dropdown-header">Big picture</li>
          <li><a href="../../../../doc/intro/html/index.html">The finite element method</a></li>
          <li><a href="../../../../doc/the_data_structure/html/index.html">The data structure</a></li>
          <li><a href="../../../../doc/quick_guide/html/index.html">Not-so-quick guide</a></li>
          <li><a href="../../../../doc/optimisation/html/index.html">Optimisation</a></li>
          <li><a href="../../../../doc/order_of_action_functions/html/index.html">Order of action functions</a></li>
          <li role="separator" class="divider"></li>
          <li class="dropdown-header">Example codes and tutorials</li>
          <li><a href="../../../../doc/example_code_list/html/index.html">List of example codes and tutorials</a></li>
          <li><a href="../../../../doc/example_code_list/html/index.html#meshes">Meshing</a></li>
          <li><a href="../../../../doc/example_code_list/html/index.html#solvers">Solvers</a></li>
          <li><a href="../../../../doc/example_code_list/html/index.html#parallel">MPI parallel processing</a></li>
          <li><a href="../../../../doc/example_code_list/html/index.html#visualisation">Post-processing/visualisation</a></li>
          <li role="separator" class="divider"></li>
          <li class="dropdown-header">Other</li>
          <li><a href="../../../../doc/change_log/html/index.html">Change log</a></li>
          <li><a href="../../../../doc/creating_doc/html/index.html">Creating documentation</a></li>
          <li><a href="../../../../doc/coding_conventions/html/index.html">Coding conventions</a></li>
          <li><a href="../../../../doc/index/html/index.html">Index</a></li>
          <li><a href="../../../../doc/FAQ/html/index.html">FAQ</a></li>
        </ul>
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Installation<span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../../../../doc/the_distribution/html/index.html">Installation guide</a></li>
            <li><a href="../../../../doc/copyright/html/index.html">Copyright</a></li>
          </ul>
        </li>
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">About <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../../../../doc/people/html/index.html">People</a></li>            
            <li><a href="../../../../doc/contact/html/index.html">Contact/Get involved</a></li>
            <li><a href="../../../../doc/publications/html/index.html">Publications</a></li>
            <li><a href="../../../../doc/acknowledgements/html/index.html">Acknowledgements</a></li>
            <li><a href="../../../../doc/picture_show/index.html">Picture show</a></li>
          </ul>
        </li>
      </li>
    </ul>
    <ul class="nav navbar-nav navbar-right navbar-search">
      <form class="navbar-form" role="search" action="../../../../doc/search_results/html/index.html">
        <div class="input-group">
          <input type="text" class="form-control" placeholder="Search" name="q">
          <span class="input-group-btn">
            <button class="btn btn-default" type="submit">Go</button>
          </span>
        </div><!-- /input-group -->
       <!--<div class="form-group">
          <input type="text" class="form-control" placeholder="Search">
        </div>
        <button type="submit" class="btn btn-default">Submit</button>-->
      </form>
    </ul>
  </div><!-- /.navbar-collapse -->
</div><!-- /.container-fluid -->
</div>
</nav>
<!-- Generated by Doxygen 1.9.2 -->
</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">Parallel processing </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p ><code>oomph-lib</code> is designed so that, provided the library is compiled with MPI support, discussed below in <a class="el" href="index.html#basics">Basic parallel usage</a>, many of the most computationally-expensive phases of a typical computation are automatically performed in parallel. Examples of "automatically parallelised" tasks include</p><ul>
<li>The assembly of the Jacobian matrix and the residual vector in Newton's method.</li>
<li>Error estimation.</li>
<li>The solution of the linear systems within the Newton iteration, and any preconditioning operations performed within <code>oomph-lib's</code> <a href="../../../mpi/block_preconditioners/html/index.html">block-preconditioning framework</a> which relies heavily on the library's <a href="../../../mpi/distributed_linear_algebra_infrastructure/html/index.html">distributed linear algebra infrastructure.</a></li>
</ul>
<p>The only parallel task that requires user intervention is the distribution of a problem over multiple processors so that each processor stores a subset of the elements. For straightforward problems, a single call to <code>Problem::distribute()</code> suffices. Furthermore, the majority of <code>oomph-lib's</code> multi-physics helper functions (<em> e.g.</em> the automatic setup of the fluid load on solid elements in FSI problems; the determination of "source elements" in multi-field problems; etc) can be used in distributed problems. For less-straightforward problems, the user may have to intervene in the distribution process and/or be aware of the consequences of the distribution. Hence, the section <a class="el" href="index.html#domain_decomposition">Distribution of problems by domain decomposition</a> provides an overview of the underlying design used for problem distribution within <code>oomph-lib</code>. A number of <a href="../../../example_code_list/html/index.html#parallel">demo driver codes for distributed problems</a> are provided and any additional issues are discussed in the accompanying tutorials.</p>
<p ><br  />
<br  />
</p><hr  />
 <hr  />
<h1><a class="anchor" id="basics"></a>
Basic parallel usage</h1>
<h2><a class="anchor" id="installation"></a>
How to build/install oomph-lib with MPI support</h2>
<ul>
<li>To compile <code>oomph-lib</code> with MPI support you must specify the configure flag <br  />
<br  />
 <div class="fragment"><div class="line">--enable-MPI</div>
</div><!-- fragment --> <br  />
 If you use <code>oomph-lib's</code> <code>autogen.sh</code> script to build the library you should add this line to the <code>config/configure_options/current</code> file. You should also ensure that appropriate parallel compilers are specified by the <code>CXX</code>, <code>CC</code>, <code>F77</code> and <code>LD</code> flags. For instance, if you use <a href="http://www.lam-mpi.org/">LAM</a>, you should use <code>CXX=mpic++</code>, <code>CC=mpicc</code>, <code>F77=mpif77</code> and <code>LD=mpif77</code>. <br  />
<br  />
</li>
<li>When <code>oomph-lib</code> is built with MPI support, the macro <code>OOMPH_HAS_MPI</code> is defined. It is used to isolate parallel sections of code to ensure that the library can be used in serial and parallel: <em> e.g. </em> <br  />
<br  />
 <div class="fragment"><div class="line">[...]</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#ifdef OOMPH_HAS_MPI</span></div>
<div class="line"> </div>
<div class="line">   std::cout &lt;&lt; <span class="stringliteral">&quot;This code has been compiled with mpi support \n &quot;</span> </div>
<div class="line">             &lt;&lt; <span class="stringliteral">&quot;and is running on &quot;</span> &lt;&lt; Communicator_pt-&gt;nproc() </div>
<div class="line">             &lt;&lt; <span class="stringliteral">&quot; processors. &quot;</span> &lt;&lt; std::endl;</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#else</span></div>
<div class="line"> </div>
<div class="line">   std::cout &lt;&lt; <span class="stringliteral">&quot;This code has been compiled without mpi support&quot;</span> </div>
<div class="line">             &lt;&lt; std::endl;</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#endif</span></div>
<div class="line"> </div>
<div class="line">[...]</div>
</div><!-- fragment --> <br  />
</li>
</ul>
<hr  />
<h2><a class="anchor" id="running"></a>
How to run a driver code in parallel</h2>
<ul>
<li>MPI <b> must </b> be initialised in every driver code that is to be run in parallel, which means that the commands <code>MPI_Helpers::init(...)</code> and <code>MPI_Helpers::finalize()</code> must be added to the beginning and end of the <code>main(...)</code> function, respectively <br  />
<br  />
 <div class="fragment"><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> **argv)</div>
<div class="line">  { </div>
<div class="line"> </div>
<div class="line">    <span class="comment">// Initialise oomph-lib&#39;s MPI       </span></div>
<div class="line">    MPI_Helpers::init(argc,argv);     </div>
<div class="line">  </div>
<div class="line"> </div>
<div class="line">    <span class="comment">// Normal &quot;serial&quot; code</span></div>
<div class="line">    [...]  </div>
<div class="line"> </div>
<div class="line">    </div>
<div class="line">    <span class="comment">// Shut down oomph-lib&#39;s MPI</span></div>
<div class="line">    MPI_Helpers::finalize();</div>
<div class="line">  }</div>
</div><!-- fragment --><br  />
 <br  />
 <br  />
 Running the code on multiple processors should immediately lead to a speedup, although this depends on the specific problem. In most applications, the most computationally-expensive tasks are the setup of the Jacobian matrix and the solution of the linear systems. When a driver code is run on multiple processors, each processor assembles contributions to the Jacobian matrix from a different subset of the elements, dividing the work of the assembly between processors. In our experience, the assembly of the Jacobian matrix tends to scale very well with the number of processors. The parallel performance of the (third-party) linear solvers available from within <code>oomph-lib</code> varies greatly and their performance is also strongly dependent on the underlying hardware, <em> e.g.</em> the speed of your machine's interconnects, etc. <br  />
<br  />
</li>
<li>The MPI header file <code>mpi.h</code> is included in <code>oomph-lib's</code> generic header, so it is not necessary to include it in the driver code. The functions <code>MPI_Helpers::init(...)</code> and <code>MPI_Helpers::finalize()</code> call their MPI counterparts, <code>MPI_Init(...)</code> and <code>MPI_Finalize()</code>, which <b>must</b> <b>not</b> be called again. Note also that the <code>main(...)</code> function must take arguments which are then passed into the <code>MPI_Helpers::init(...)</code> function. <br  />
<br  />
</li>
<li>The command used to run a parallel job depends on your particular MPI installation. If you use <a href="http://www.open-mpi.org/">OpenMPI</a>, for example, the executable <code>parallel_executable</code> is run on, say, four processors by issuing the command <br  />
<br  />
 <div class="fragment"><div class="line">mpirun -np 4 ./parallel_executable</div>
</div><!-- fragment --> <br  />
</li>
</ul>
<hr  />
<h2><a class="anchor" id="solvers"></a>
oomph-lib's parallel linear solvers</h2>
<ul>
<li><code>oomph-lib's</code> default linear solver is <code>SuperLUSolver</code>. This is a wrapper to the direct linear solvers from the <a href="http://crd.lbl.gov/~xiaoye/SuperLU">SuperLU / SuperLU_DIST</a> projects. If <code>oomph-lib</code> is built with MPI support and the executable is run on multiple processors, <code>SuperLU_DIST</code> will be used by default, otherwise <code>SuperLU</code> is used. <br  />
<br  />
</li>
<li>Of <code>oomph-lib's</code> own iterative linear solvers, only <code>CG</code> is parallelised. We recommend using <code>oomph-lib's</code> wrapper to the parallel Krylov subspace solvers from the <a href="http://trilinos.sandia.gov/">Trilinos </a> library (see <a href="../../../the_distribution/html/index.html#external_dist">the <code>oomph-lib</code> installation page</a> for details on how to install this) instead. The interfaces are identical to those used to call these solvers in serial; see <a href="../../../linear_solvers/html/index.html">the linear solver tutorial</a> for details. <br  />
<br  />
</li>
<li><code>oomph-lib's</code> block preconditioning framework is fully parallelised and can be used in the same way as in a serial code.</li>
</ul>
<hr  />
<h2><a class="anchor" id="self_tests"></a>
How to include parallel demo codes into the self-tests</h2>
<ul>
<li>The configure flag <code>&ndash;with-mpi-self-tests</code> includes <code>oomph-lib's</code> parallel demo driver codes into the self-tests executed when <code>make</code> <code>check</code> is run. The self-tests require the executable to be run on two processors and the command that spawns a two-processor parallel job on the target machine must be specified as an argument to the configure flag. For example, under <a href="http://www.lam-mpi.org/">LAM</a> <br  />
<br  />
 <div class="fragment"><div class="line">--with-mpi-<span class="keyword">self</span>-tests=<span class="stringliteral">&quot;mpirun -np 2&quot;</span></div>
</div><!-- fragment --> <br  />
 Some self-tests are performed with a greater number of processors. To perform these tests as part of the <code>make</code> <code>test</code> procedure, add the configure flag <code>&ndash;with-mpi-self-tests-variablenp</code> to the configure options. Its argument has to specify how to spawn an mpi job on an arbitrary number of processors, using the placeholder <code>OOMPHNP</code> for the number of processors. E.g. <br  />
<br  />
 <div class="fragment"><div class="line">--with-mpi-<span class="keyword">self</span>-tests-variablenp=<span class="stringliteral">&quot;mpirun -np OOMPHNP&quot;</span></div>
</div><!-- fragment --> <br  />
 It is easiest to add the appropriate lines to the <code>config/configure_options/current</code> file before building/installing <code>oomph-lib</code> with <code>autogen.sh</code>. <br  />
<br  />
 <b>NOTE:</b> When using LAM, make sure your MPI demons are started before running the parallel self-tests, e.g. by using the <code>lamboot</code> command, otherwise the self-tests will fail.</li>
</ul>
<hr  />
 <hr  />
<h1><a class="anchor" id="domain_decomposition"></a>
Distribution of problems by domain decomposition</h1>
<p >By default each processor stores the entire <code>Problem</code> object, which means that all data is available on all processors. As a result the size of the problem is limited by the smallest amount of memory available on any of the processors. In addition, the mesh adaptation does not benefit from parallel processing because each processor must adapt its own copy of the entire mesh, even though it operates on a subset of the elements when assembling the Jacobian matrix.</p>
<p >To address this problem, <code>oomph-lib's</code> domain decomposition procedures allow a <code>Problem</code> to be distributed over multiple processors so that each processor holds a fraction of the <code>Problem's</code> elements, which can lead to substantial reductions in memory usage per processor and allows the mesh adaptation to be performed in parallel.</p>
<hr  />
<h2><a class="anchor" id="how_to_distribute"></a>
Basic usage: How to distribute a simple problem</h2>
<ul>
<li>In most cases the problem distribution is extremely straightforward. Given an existing serial driver code, modified by the addition of calls to <code>MPI_Helpers::init(...)</code> and <code>MPI_Helpers::finalize()</code>, the function <br  />
<br  />
 <div class="fragment"><div class="line">Problem::distribute()</div>
</div><!-- fragment --> <br  />
 can be called at any point <b>after</b> the <code>Problem</code> has been constructed and equation numbers have been assigned (i.e. at a point at which <code>Problem::newton_solve()</code> could be called), but <b>before</b> any <b>non</b>-uniform mesh refinement has taken place. Equation numbering is required because the automatic distribution procedure uses the global equation numbering scheme to identify interactions between elements and attempts to store strongly-coupled elements on the same processor. <br  />
<br  />
 After the call to <code>Problem::distribute()</code> each processor holds a sub-set of the <code>Problem's</code> elements: those elements whose contribution to the Jacobian are assembled by the processor, and additional "halo" elements that are retained to facilitate the subsequent mesh adaptation. (Halo elements are discussed in the <a class="el" href="index.html#how_it_works">Overview of the implementation of Problem distribution</a> below). We note that the meshes' boundary lookup schemes are updated during the distribution process so that <code>Mesh::nboundary_element(b)</code> returns the number of elements on the processor that are adjacent to boundary <code>b</code>. Hence, functions that were written (in serial) to update time-dependent boundary conditions on a mesh's boundary nodes, say, continue to work in parallel without requiring any modifications. <br  />
<br  />
</li>
<li>Now that each processor holds a subset of the <code>Problem's</code> elements, it is sensible to modify the post-processing routines such that output files are labelled by the processor number: <br  />
<br  />
  <div class="fragment"><div class="line"><span class="comment">//==start_of_doc_solution=================================================</span></div>
<div class="line"><span class="comment">/// Doc the solution</span></div>
<div class="line"><span class="comment">//========================================================================</span></div>
<div class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> ELEMENT&gt;</div>
<div class="line"><span class="keywordtype">void</span> RefineableDrivenCavityProblem&lt;ELEMENT&gt;::doc_solution(DocInfo&amp; doc_info)</div>
<div class="line">{ </div>
<div class="line"> </div>
<div class="line"> ofstream some_file;</div>
<div class="line"> <span class="keywordtype">char</span> filename[100];</div>
<div class="line"> </div>
<div class="line"> <span class="comment">// Number of plot points</span></div>
<div class="line"> <span class="keywordtype">unsigned</span> npts=5; </div>
<div class="line"> </div>
<div class="line"> <span class="comment">// Get current process rank</span></div>
<div class="line"> <span class="keywordtype">int</span> my_rank=this-&gt;communicator_pt()-&gt;my_rank();</div>
<div class="line"> </div>
<div class="line"> <span class="comment">// Output solution </span></div>
<div class="line"> sprintf(filename,<span class="stringliteral">&quot;%s/soln%i_on_proc%i.dat&quot;</span>,doc_info.directory().c_str(),</div>
<div class="line">         doc_info.number(),my_rank);</div>
<div class="line"> some_file.open(filename);</div>
<div class="line"> mesh_pt()-&gt;output(some_file,npts);</div>
<div class="line"> some_file.close();</div>
<div class="line"> </div>
<div class="line">} <span class="comment">// end_of_doc_solution</span></div>
</div><!-- fragment --> <br  />
 Without such a modification multiple processors will attempt to write to the same file, leading to incomplete output if/when the file produced by one processor is overwritten by another. By default, <code>oomph-lib's</code> mesh-based output functions do not include output from halo elements; this can be re-enabled or disabled by calls to <br  />
<br  />
 <div class="fragment"><div class="line">Mesh::enable_output_of_halo_elements()</div>
</div><!-- fragment --> <br  />
 or <br  />
<br  />
 <div class="fragment"><div class="line">Mesh::disable_output_of halo_elements()</div>
</div><!-- fragment --> <br  />
 respectively. <br  />
<br  />
</li>
<li>Other, equally-straightforward modifications to existing driver codes tend to be required if the serial version of the code contains explicit references to individual elements or nodes, <em> e.g. </em> pinning a pressure value in a Navier&ndash;Stokes computation with enclosed boundaries. In such cases, it is important to remember that, once the problem is distributed, <b></b>(i) not every processor has direct access to a specific element (or node), and <b></b>(ii) the pointer to the "first" element in a mesh (say) points to a different element on each processor. The particular example of pinning a pressure degree of freedom in a Navier&ndash;Stokes problem is discussed in detail in the <a href="../../adaptive_driven_cavity/html/index.html">adaptive driven cavity tutorial</a>.</li>
</ul>
<hr  />
<h2><a class="anchor" id="how_it_works"></a>
Overview of the implementation of Problem distribution</h2>
<p >The main task of the <code>Problem::distribute()</code> function is to distribute the <code>Problem's</code> global mesh (possibly comprising multiple sub-meshes) amongst the processors so that <b>(a)</b> the storage requirements on each processor are reduced; <b>(b)</b> during the mesh adaptation each processor only acts on a fraction of the overall mesh; while ensuring that <b>(c)</b> communication between processors required to synchronise any shared data structures is minimised.</p>
<p >The figure below shows a conceptual sketch of the parallelisation strategy adopted. For simplicity, we shall restrict the discussion to the 2D case and ignore various complications that arise with more complicated mesh partitionings.</p>
<p >The initial distribution of a problem proceeds in two stages:</p>
<ul>
<li><b>Initial refinement and partitioning</b> <br  />
<br  />
 Each processor constructs the same <code>Problem</code> object, using a (typically very coarse) initial mesh; in the figure below, the mesh contains a single four-node quad. Repeated calls to <code>Problem::refine_uniformly()</code> should be made to increase the number of elements sufficiently for a sensible mesh partitioning &mdash; for example, there should be at least as many elements as processors. By default, <a href="http://www-users.cs.umn.edu/~karypis/metis/">METIS</a> is used to associate each element with a unique processor. Alternatively, user-defined distributions may be specified via a vector that contains the processor number to be associated with each element. Nodes located in the interior of a processor's patch of elements are associated with that processor; nodes shared by elements associated with different processors are associated with the highest-numbered processor. <br  />
<br  />
</li>
<li><b>Identification of halo[ed] nodes/elements and pruning</b> <br  />
<br  />
 The elements and nodes required by each processor must now be determined. Each processor retains its own elements: those elements associated with the processor by the partitioning process. In addition, each processor retains a single layer of elements adjacent to its own elements, and their nodes. Nodes that lie directly on the boundary between this layer and the processor's own elements are shared between processors and are associated with the highest-numbered processor, as explained above. These additional elements/nodes that are retained but not associated with the processor are termed <code>halo'' elements/nodes. Conversely, objects are termed</code>haloed'' if they are associated with the processor, but they have halo counterparts on other processors. [It is possible to request that all elements in a mesh are retained as halo elements. This is useful in certain free-boundary problems; see the section <a class="el" href="index.html#alg_node_update">Distributing problems involving meshes with algebraic node updates</a> below for details]. <br  />
<br  />
 At this stage of the process, each processor has access to the entire mesh and it is, therefore, possible to establish a consistent numbering scheme for halo[ed] elements/nodes. Once this information has been set up, any superfluous nodes and elements are deleted and the mesh's boundary lookup-schemes (required to identify the nodes and elements located adjacent to domain boundaries) and neighbour information for adaptive refinement are re-generated. Finally, each processor independently assigns equation numbers for its associated (non-halo) elements and nodes; the equation numbers are then synchronised between processors.</li>
</ul>
<div class="image">
<img src="decomposition.gif" alt=""/>
<div class="caption">
Sketch illustrating the phases of the parallel mesh adaptation procedure for a problem that is distributed over four processors. The columns illustrate the evolution of the mesh on each of the four processors. The colours of the objects indicate which processor is associated with them. </div></div>
 <p >Aside from the initial refinement process, the functionality described above is implemented in a single function, <code>Problem::distribute()</code>. Following its execution on all processors, each processor can assemble its contribution to the distributed Jacobian matrix and residual vector, required by <code>oomph-lib's</code> parallel linear solvers, using only to locally stored non-halo objects. Once the Newton correction to the unknowns has been computed, each processor updates the unknowns associated with its elements and nodes, before MPI-based communication is employed to update the unknowns stored at the processors' halo nodes.</p>
<p >After a problem has been distributed, further mesh refinement can be performed in parallel using the existing mesh adaptation procedures on the (partial) meshes held on the different processors. For spatially non-uniform refinement, each haloed element communicates whether or not it is to be refined to its halo counterparts before the adaptation takes place. Any nodes created during the refinement are associated with a unique processor, using the rules described above, and halo[ed] nodes are identified and added to the appropriate lookup schemes. These steps are performed automatically when <code>Problem::refine_uniformly()</code> or any of the other mesh adaptation routines within <code>oomph-lib</code> are executed.</p>
<p ><b> Optional pruning of superfluous halo[ed] nodes and elements </b></p>
<p >The parallel efficiency of the distributed mesh adaptation (in terms of the memory required to hold the partial meshes, and in terms of the CPU time required for their adaptation) is limited by the fact that each processor must adapt not only the <img class="formulaInl" alt="$N_{\rm in \ charge}$" src="form_0.png" width="50" height="14"/> elements it is in charge of, but also its <img class="formulaInl" alt="$N_{\rm halo}$" src="form_1.png" width="29" height="13"/> halo elements. We define the efficiency of the problem distribution as </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ e_{dist} = \frac{N_{\rm in \ charge}}{N_{\rm halo} + N_{\rm in \ charge}} \le 1, \]" src="form_2.png" width="164" height="30"/>
</p>
<p> where the equality could only be achieved in the absence of any halo elements.</p>
<p >When the mesh is first distributed, the halo layer has a depth of one element, but repeated mesh refinement can make the halo layers (the original halo elements and their sons) much thicker than a single-element layer. Thus, to a large extent, the efficiency is determined during the initial problem distribution and at that stage of the process it can only be improved by <b>(i)</b> increasing the number of non-distributed initial mesh refinements; <b>(ii)</b> reducing the number of processors. Since both options reduce the parallelism they are not desirable. It is possible, however, to improve the parallel efficiency by pruning superfluous halo[ed] elements after each mesh refinement by calling the function <br  />
<br  />
</p><div class="fragment"><div class="line">Problem::prune_halo_elements_and_nodes()</div>
</div><!-- fragment --><p> <br  />
as illustrated in the figure. If this is done after every mesh adaptation <img class="formulaInl" alt="$ e_{dist}$" src="form_3.png" width="23" height="9"/> increases significantly as the refinement proceeds. However, the pruning of halo[ed] nodes and elements makes the refinement irreversible and the mesh(es) involved can no longer be unrefined below the previous highest level of uniform refinement.</p>
<hr  />
<h2><a class="anchor" id="advanced"></a>
Customising the distribution</h2>
<p >The procedures described above are completely sufficient for straightforward problems, <em> e.g. </em> a single mesh containing single-physics elements. For less-straightforward problems, <em> e.g. </em> those that involve interactions between multiple meshes, the interactions must be set up both <b> before </b> and <b> after </b> the distribution. The functions <br  />
<br  />
</p><div class="fragment"><div class="line">Problem::actions_before_distribute()</div>
</div><!-- fragment --><p> <br  />
and <br  />
<br  />
</p><div class="fragment"><div class="line">Problem::actions_after_distribute()</div>
</div><!-- fragment --><p> <br  />
can be used to perform any additional commands required to complete the setup of the problem after distribution. In many cases, these functions will contain the same commands as those required in the equivalent <code>Problem::actions_before_adapt()</code> and <code>Problem::actions_after_adapt()</code> functions used during mesh adaptation.</p>
<hr  />
<h3><a class="anchor" id="face_elements"></a>
Distributing problems involving FaceElements</h3>
<p ><code>FaceElements</code> are typically used to apply Neumann/traction-type boundary conditions; see the tutorials that discuss the application of such boundary conditions in <a href="../../../poisson/two_d_poisson_flux_bc/html/index.html">Poisson</a> or <a href="../../../navier_stokes/rayleigh_traction_channel/html/index.html">Navier-Stokes</a> equations. Since the <code>FaceElements</code> that apply the Neumann boundary conditions are attached to "bulk" elements that may disappear during mesh adaptation, we generally recommend to store the (pointers to the) <code>FaceElements</code> in a separate mesh, and to use the <code>Problem::actions_before_adapt()</code> and <code>Problem::actions_after_adapt()</code> functions to detach and re-attach the <code>FaceElements</code> to/from the bulk elements before and after the mesh adaptation.</p>
<p >The same issues arise during the problem distribution: A <code>FaceElement</code> that was created before the problem was distributed may have been attached to a bulk element that is deleted when the distribution is performed, resulting in obvious (and disastrous) consequences. We therefore recommend using the functions <br  />
<br  />
</p><div class="fragment"><div class="line">Problem::actions_before_distribute()</div>
</div><!-- fragment --><p> <br  />
and <br  />
<br  />
</p><div class="fragment"><div class="line">Problem::actions_after_distribute()</div>
</div><!-- fragment --><p> <br  />
to detach and re-attach any <code>FaceElements</code> before and after the problem distribution. In this context it is important to note that: <br  />
</p><ol type="1">
<li>The <code>FaceElements</code> <b>should</b> be available before <code>Problem::distribute()</code> is called to allow the load-balancing routines to take their presence into account. <br  />
<br  />
</li>
<li><code>FaceElements</code> that are attached to halo (bulk-)elements become halo-elements themselves.</li>
</ol>
<p>Further details are provided in <br  />
 <a href="../../two_d_poisson_flux_bc_adapt/html/index.html">another tutorial</a> which explains the modifications to the serial driver code required to distribute a Poisson problem with Neumann boundary conditions.</p>
<hr  />
<h3><a class="anchor" id="multi_domain"></a>
Distributing multi-domain problems</h3>
<p >Multi-domain problems involve interactions between PDEs that are defined in different domains, such as fluid-structure interaction problems. Within <code>oomph-lib</code>, multi-domain problems typically involve elements, derived from the <code>ElementWithExternalElement</code> class, <br  />
 that store pointers to any "external" elements that take part in the interaction. These "external" elements are determined by helper functions such as <code>FSI_functions::setup_fluid_load_info_for_solid_elements(...)</code> or <code>Multi_domain_functions::setup_multi_domain_interactions(...)</code>. The appropriate helper functions must be called in the function <code>Problem::actions_after_distribute()</code> to ensure that the interactions are correctly set up once the problem has been distributed.</p>
<p >The helper function <code>Multi_domain_functions::locate_external_elements()</code> has been written to work even after a problem has been distributed and uses the following algorithm:</p><ol type="1">
<li>Loop over all (non-halo) <code>ElementWithExternalElements</code> and try to locate the "external" elements (<em> e.g.</em> fluid elements adjacent to an elastic wall in an fluid-structure interaction problem) on the current processor. If the required "external" element is found locally, the <code>ElementWithExternalElement</code> stores a pointer to it. <br  />
<br  />
</li>
<li>If the "external" element cannot be found locally, MPI-based communication is employed to find the "external" element on one of the other processors. Once found, a halo-copy of the "external" element (and its nodes) is made on the current processor and a pointer to the halo-element is stored. These "external" halo elements and nodes are stored in the appropriate mesh, <em> i.e.</em> in an FSI problem, the "external" fluid elements are added to the fluid mesh. <br  />
<br  />
</li>
</ol>
<p>"External" halo[ed] elements are automatically included in any halo/haloed synchronisation operations performed when assigning equation numbers, or updating unknowns during the Newton iteration, etc.</p>
<p >The procedure discussed above has the following important consequence: <br  />
</p><ul>
<li>When an "external" halo element is created we also automatically create halo-copies of those of its nodes that do not already exist on the current processor. Such nodes are stored as "external" halo nodes and they are automatically synchronised with their non-halo counterparts on other processors. However, synchronisation of nodes does not (and cannot) include the specification of auxiliary node update functions (such as the function <code>FSI_functions::apply_no_slip_on_moving_wall(...)</code> which automatically applies the no-slip condition on moving fluid-solid interfaces). Such functions should therefore be re-assigned to the appropriate nodes after <code>FSI_functions::setup_fluid_load_info_for_solid_elements()</code> has been called. This is exactly equivalent to the sequence of steps required following an adaptive mesh refinement; see <em> e.g.</em> the <a href="../../../interaction/fsi_collapsible_channel_adapt/html/index.html#before_and_after">tutorial discussing the adaptive solution of the collapsible channel problem</a> for a more detailed discussion of this issue. We note that "external" halo-nodes are added to the mesh's boundary lookup schemes, so the specification of auxiliary node update functions for all nodes on a given mesh boundary does not require any further modification to the serial code. <br  />
</li>
</ul>
<hr  />
<h3><a class="anchor" id="alg_node_update"></a>
Distributing problems involving meshes with algebraic node updates</h3>
<p ><code>oomph-lib</code> provides a variety of algebraic node-update methods. These allow the fast and sparse update of the nodal positions in response to changes in the domain boundaries. The shape and position of such boundaries is typically represented by one or more <code>GeomObjects</code>. If the motion of the boundary is prescribed, (as in the case of the <a href="../../../navier_stokes/osc_ellipse/html/index.html">flow inside an oscillating ellipse</a>, say) no modifications are required when the meshes are used in a distributed problem.</p>
<p >In order to minimise communication, the design decision was taken that any <code>GeomObjects</code> defining the position of domain boundaries must be available on all processors after the problem is distributed. Thus, if the <code>GeomObject</code> is actually a <code>MeshAsGeomObject</code>, a compound <code>GeomObject</code> formed from a mesh of <code>FiniteElements</code>, then all the elements in the mesh, or all elements required to construct the mesh, must be retained as halo elements on every processor. This leads to a slight increase in the overall storage requirements (because none of the elements involved in the interaction are deleted when the problem is distributed) but it means that the entire <code>GeomObject</code> remains accessible to the fluid mesh without invoking MPI communications. Two functions can be used to specify that elements must be retained: <br  />
<br  />
</p><div class="fragment"><div class="line">Mesh::keep_all_elements_as_halos()</div>
</div><!-- fragment --><p> <br  />
keeps every element in the <code>Mesh</code> available to every processor, and </p><div class="fragment"><div class="line">GeneralisedElement::must_be_kept_as_halo()</div>
</div><!-- fragment --><p> <br  />
can be called for a particular element to ensure that it is kept available to every processor.</p>
<p >We stress that the increase in storage requirements due to the retention of these elements is minimal because the <br  />
 elements are only located along the (lower-dimensional) boundaries of the domain. For instance, in the <a href="../../../interaction/fsi_collapsible_channel_algebraic/html/index.html">collapsible channel problem</a> the 1D mesh of beam elements bounds the 2D mesh of fluid elements; in <a href="../../../interaction/turek_flag/html/index.html">Turek and Hron's FSI benchmark problem</a>, the 2D fluid domain is bounded by a 1D mesh of <code>FSISolidTractionElements</code>, and so on.</p>
<p >Examples of the implementation of these ideas are given for the <br  />
 <a href="../../fsi_channel_with_leaflet/html/index.html">flow past an elastic leaflet</a> and <a href="../../turek_flag/html/index.html">Turek and Hron's FSI benchmark problem </a>.</p>
<hr  />
 <hr  />
<h1><a class="anchor" id="mpidetails"></a>
Further MPI Details</h1>
<ul>
<li><code>oomph-lib</code> mirrors the MPI C bindings with the methods <code>MPI_Helpers::init(...)</code> and <code>MPI_Helpers::finalize()</code>; they call the methods <code>MPI_Init(...)</code> and <code>MPI_Finalize()</code> respectively. In addition, these methods automatically create (and destroy) a new instance of <code>MPI_Comm</code> with the same set of processes as <code>MPI_COMM_WORLD</code> but with a different communication context. This <code>MPI_Comm</code> instance is accessible through <code>MPI_Helpers::Communicator_pt</code> which returns a pointer to an <code>OomphCommunicator</code> object.</li>
<li>An <code>OomphCommunicator</code> is <code>oomph-lib's</code> object oriented wrapper to an <code>MPI_Comm</code>.</li>
<li>Under normal operation, a user does not need to specify the <code>OomphCommunicator</code> for any object &ndash; this is all handled automatically by <code>oomph-lib</code>. For example, on construction a <code>Problem</code> will use the <code>MPI_Helpers</code> communicator; a <code>LinearSolver</code> will use the corresponding <code>Problem</code> communicator; and a <code>Preconditioner</code> will use the corresponding <code>IterativeLinearSolver</code> communicator.</li>
</ul>
<hr  />
 <hr  />
<h1><a class="anchor" id="problem"></a>
Trouble-shooting and debugging</h1>
<h2><a class="anchor" id="checking_and_documenting"></a>
Debugging and documenting the distribution</h2>
<p >Once a problem has been distributed, the function </p><div class="fragment"><div class="line">Problem::check_halo_schemes()</div>
</div><!-- fragment --><p> can be called to check that the halo lookup schemes for each mesh are set up correctly.</p>
<p >Details about the mesh distribution can be generated by calling </p><div class="fragment"><div class="line">Mesh::doc_mesh_distribution(DocInfo&amp; doc_info)</div>
</div><!-- fragment --><p> which outputs the elements, nodes, halo(ed) elements, halo(ed) nodes, mesh, boundary elements and boundary nodes on each processor. This routine is automatically called when <code>Problem::distribute()</code> is called with a <code>DocInfo</code> object whose <code>Doc_flag</code> is set to true (the default behaviour).</p>
<hr  />
<h2><a class="anchor" id="parallel_debug"></a>
Debugging parallel code</h2>
<p >Parallel code can obviously fail in many more ways than a code that runs on a single processor. Here is a procedure that allows basic parallel debugging without requiring access to expensive commercial tools such as totalview, say. (The instructions below assume that you use <a href="http://www.lam-mpi.org/">LAM</a> as your MPI installation; they can probably be modified to work with other versions of MPI, too).</p>
<p >Let's assume you use <a href="http://www.gnu.org/software/gdb/">gdb</a> as your debugger. To debug a serial code with <a href="http://www.gnu.org/software/gdb/">gdb</a> you would load the executable <code>a.out</code> into the debugger using the command </p><div class="fragment"><div class="line">gdb ./a.out</div>
</div><!-- fragment --><p> on the command line. Once inside <a href="http://www.gnu.org/software/gdb/">gdb</a>, you run the code by typing "run". If the code crashes, typing "where" will tell you in which line of the code the crash occurred, and it will also provide a traceback of the function calls that got you to this point.</p>
<p >To do this in parallel, we have to run each (parallel) instance of the code within its own <a href="http://www.gnu.org/software/gdb/">gdb</a> session. To this, create the following three files:</p><ul>
<li>A shell script <code>mpidbg</code> that must be executable, which contains: <div class="fragment"><div class="line">mpirun $1 $2 -x DISPLAY rungdb.sh -x cmds.gdb</div>
</div><!-- fragment --></li>
<li>The executable file <code>rungdb.sh</code> must be in the same directory as <code>mpidbg</code> and contain the following <div class="fragment"><div class="line">echo <span class="stringliteral">&quot;Running GDB on node `hostname`&quot;</span></div>
<div class="line">echo $DISPLAY</div>
<div class="line">xterm -geometry 200x40 -e gdb $*</div>
<div class="line">exit 0</div>
</div><!-- fragment --></li>
<li>Finally the <code>cmds.gdb</code> file should also be in the same directory and must contain the <code>run</code> command (so that all processors start their <a href="http://www.gnu.org/software/gdb/">gdb</a> session simultaneously), and may also include other commands such as <code>set</code> <code>args</code> <code>command-line-arguments</code>, and so on.</li>
</ul>
<p >Then, to run the debugger in parallel on 3 processors for the executable <code>a.out</code>, the command would be </p><div class="fragment"><div class="line">mpidbg -np 3 ./a.out </div>
</div><!-- fragment --><p> Once the command is issued, an <code>xterm</code> window will be opened for each processor, and if a crash occurs on any processor, the usual <a href="http://www.gnu.org/software/gdb/">gdb</a> commands (<code>back</code>, <code>up</code>, <code>down</code>, <code>quit</code> and so on) may be used within any of the <code>xterm</code> sessions where a crash has taken place.</p>
<hr  />
 <hr  />
 <h1><a class="anchor" id="pdf"></a>
PDF file</h1>
<p >A <a href="../latex/refman.pdf">pdf version</a> of this document is available. </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->

    <!-- jQuery for Bootstrap and Doxygen -->
    <script src="../../../js/jquery-1.12.0.min.js"></script>
    <!-- Minified boostrap plugins-->
    <script src="../../../js/bootstrap.js"></script>
    <!-- Doxygen dependency to add powertips to source code-->
    <script src="../../../js/jquery.powertip.min.js"></script>
    <!-- The  following script is generated by doxygen and hides/shows levels in 
         the data structure lists and adds powertips to source code-->
    <script src="../../../js/dynsections.js" ></script>
    <!-- add to Doxygen's class names so bootstrap css and js recognises them-->
    <script type="text/javascript">
    $(".contents").addClass("container");
    $(".header").addClass("container");
    $(".navpath").addClass("container");
    $("#navrow3").addClass("container");
    $("#navrow4").addClass("container");
    $(".mlabel").addClass("label");
    $(".mlabel").addClass("label-default");
    $(".memitem").addClass("panel");
    $(".memitem").addClass("panel-info");
    $(".memproto").addClass("panel-heading");
    $(".memdoc").addClass("panel-body");
    </script>
    <footer>
      <div class="container">
        <div class="text-muted" style="float:right;">Generated by <a href="http://www.doxygen.org/index.html">
          <img style="height:18px;" class="footer-img" src="doxygen.png" alt="doxygen"></a> on Mon May 9 2022 10:22:00
        </div>
      </div>
    </footer>
</body>
</html>
